# Root Cargo.toml â€” workspace
[workspace]
resolver = "2"
members = [
    "crates/hf-providers-core",
    "crates/hf-providers-cli",
]

[workspace.package]
version = "0.1.0"
edition = "2021"
license = "MIT OR Apache-2.0"
repository = "https://github.com/YOUR_NAME/hf-providers"
description = "Find out how to run any Hugging Face model â€” providers, variants, pricing, status"

[workspace.dependencies]
reqwest = { version = "0.12", features = ["json", "rustls-tls"], default-features = false }
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
thiserror = "2"
tracing = "0.1"





















// crates/hf-providers-core/src/lib.rs

pub mod api;
pub mod model;
pub mod provider;
pub mod pricing;
pub mod snippet;
pub mod error;

pub use model::{Model, ModelVariant};
pub use provider::{Provider, ProviderStatus, ProviderInfo};
pub use error::HfpError;

// â”€â”€â”€ crates/hf-providers-core/src/error.rs â”€â”€â”€

#[derive(Debug, thiserror::Error)]
pub enum HfpError {
    #[error("HTTP request failed: {0}")]
    Http(#[from] reqwest::Error),
    #[error("Model not found: {0}")]
    ModelNotFound(String),
    #[error("JSON parse error: {0}")]
    Json(#[from] serde_json::Error),
    #[error("No HF token found. Set $HF_TOKEN or run `huggingface-cli login`")]
    NoToken,
    #[error("{0}")]
    Other(String),
}

pub type Result<T> = std::result::Result<T, HfpError>;

// â”€â”€â”€ crates/hf-providers-core/src/provider.rs â”€â”€â”€

use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "lowercase")]
pub enum ProviderStatus {
    Live,
    Staging,
    #[serde(other)]
    Unknown,
}

/// What we know about a single provider serving a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderInfo {
    pub name: String,
    pub status: ProviderStatus,
    pub task: String,
    pub provider_id: String,
    // Enriched from pricing page (optional)
    pub input_price_per_m: Option<f64>,
    pub output_price_per_m: Option<f64>,
    pub throughput_tps: Option<f64>,
    pub latency_s: Option<f64>,
    pub context_window: Option<u64>,
    pub supports_tools: Option<bool>,
    pub supports_structured: Option<bool>,
}

impl ProviderInfo {
    /// Inferred readiness from available data
    pub fn readiness(&self) -> Readiness {
        match self.status {
            ProviderStatus::Staging => Readiness::Unavailable,
            ProviderStatus::Unknown => Readiness::Unavailable,
            ProviderStatus::Live => {
                if self.latency_s.is_some() && self.throughput_tps.is_some() {
                    Readiness::Hot
                } else if self.latency_s.is_some() || self.throughput_tps.is_some() {
                    Readiness::Warm
                } else {
                    Readiness::Cold
                }
            }
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum Readiness {
    Hot,       // ğŸŸ¢ live with metrics
    Warm,      // ğŸŸ¡ live, partial metrics
    Cold,      // âš« live but no metrics (likely cold start)
    Unavailable, // âŒ staging or unknown
}

impl std::fmt::Display for Readiness {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Readiness::Hot => write!(f, "ğŸŸ¢ hot"),
            Readiness::Warm => write!(f, "ğŸŸ¡ warm"),
            Readiness::Cold => write!(f, "âš« cold"),
            Readiness::Unavailable => write!(f, "âŒ unavail"),
        }
    }
}

/// Static provider registry
#[derive(Debug, Clone)]
pub struct Provider {
    pub id: &'static str,
    pub display_name: &'static str,
    pub kind: ProviderKind,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ProviderKind {
    /// Third-party GPU backend routed through HF
    InferenceProvider,
    /// HF's own CPU-based inference
    HfInference,
}

pub const PROVIDERS: &[Provider] = &[
    Provider { id: "cerebras",     display_name: "Cerebras",     kind: ProviderKind::InferenceProvider },
    Provider { id: "cohere",       display_name: "Cohere",       kind: ProviderKind::InferenceProvider },
    Provider { id: "fal-ai",       display_name: "fal",          kind: ProviderKind::InferenceProvider },
    Provider { id: "featherless-ai", display_name: "Featherless", kind: ProviderKind::InferenceProvider },
    Provider { id: "fireworks-ai", display_name: "Fireworks",    kind: ProviderKind::InferenceProvider },
    Provider { id: "groq",         display_name: "Groq",         kind: ProviderKind::InferenceProvider },
    Provider { id: "hyperbolic",   display_name: "Hyperbolic",   kind: ProviderKind::InferenceProvider },
    Provider { id: "nebius",       display_name: "Nebius",       kind: ProviderKind::InferenceProvider },
    Provider { id: "novita",       display_name: "Novita",       kind: ProviderKind::InferenceProvider },
    Provider { id: "nscale",       display_name: "Nscale",       kind: ProviderKind::InferenceProvider },
    Provider { id: "ovhcloud",     display_name: "OVHcloud",     kind: ProviderKind::InferenceProvider },
    Provider { id: "publicai",     display_name: "Public AI",    kind: ProviderKind::InferenceProvider },
    Provider { id: "replicate",    display_name: "Replicate",    kind: ProviderKind::InferenceProvider },
    Provider { id: "sambanova",    display_name: "SambaNova",    kind: ProviderKind::InferenceProvider },
    Provider { id: "scaleway",     display_name: "Scaleway",     kind: ProviderKind::InferenceProvider },
    Provider { id: "together",     display_name: "Together AI",  kind: ProviderKind::InferenceProvider },
    Provider { id: "wavespeed",    display_name: "WaveSpeed",    kind: ProviderKind::InferenceProvider },
    Provider { id: "zai-org",      display_name: "Z.ai",         kind: ProviderKind::InferenceProvider },
    Provider { id: "hf-inference", display_name: "HF Inference", kind: ProviderKind::HfInference },
];

// â”€â”€â”€ crates/hf-providers-core/src/model.rs â”€â”€â”€

use serde::{Deserialize, Serialize};
use crate::provider::ProviderInfo;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Model {
    pub id: String,
    pub pipeline_tag: Option<String>,
    pub likes: u64,
    pub downloads: u64,
    pub inference_status: Option<String>, // "warm" or None
    pub providers: Vec<ProviderInfo>,
    pub variants: Vec<ModelVariant>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelVariant {
    pub id: String,
    pub pipeline_tag: Option<String>,
    pub likes: u64,
    pub provider_count: usize,
    pub param_hint: Option<String>, // "70B", "7B", etc. parsed from name
}

impl Model {
    pub fn cheapest(&self) -> Option<&ProviderInfo> {
        self.providers.iter()
            .filter(|p| p.output_price_per_m.is_some())
            .min_by(|a, b| {
                a.output_price_per_m.unwrap()
                    .partial_cmp(&b.output_price_per_m.unwrap())
                    .unwrap_or(std::cmp::Ordering::Equal)
            })
    }

    pub fn fastest(&self) -> Option<&ProviderInfo> {
        self.providers.iter()
            .filter(|p| p.throughput_tps.is_some())
            .max_by(|a, b| {
                a.throughput_tps.unwrap()
                    .partial_cmp(&b.throughput_tps.unwrap())
                    .unwrap_or(std::cmp::Ordering::Equal)
            })
    }

    pub fn hot_providers(&self) -> Vec<&ProviderInfo> {
        use crate::provider::Readiness;
        self.providers.iter()
            .filter(|p| p.readiness() == Readiness::Hot)
            .collect()
    }

    pub fn with_tools(&self) -> Vec<&ProviderInfo> {
        self.providers.iter()
            .filter(|p| p.supports_tools == Some(true))
            .collect()
    }

    /// Extract a likely param size from model name, e.g. "70B", "1.5B"
    pub fn param_hint(name: &str) -> Option<String> {
        let re_patterns = ["671B", "405B", "236B", "135B", "120b", "109B",
            "80B", "72B", "70B", "32B", "30B", "27B", "22B", "20B",
            "14B", "13B", "12B", "9B", "8B", "7B", "4B", "3B", "2B", "1.5B", "1.3B", "1B", "0.3B"];
        let upper = name.to_uppercase();
        for pat in &re_patterns {
            if upper.contains(&pat.to_uppercase()) {
                return Some(pat.to_string());
            }
        }
        None
    }
}

// â”€â”€â”€ crates/hf-providers-core/src/api.rs â”€â”€â”€

use crate::error::{HfpError, Result};
use crate::model::{Model, ModelVariant};
use crate::provider::{ProviderInfo, ProviderStatus};
use reqwest::Client;
use serde_json::Value;

const HF_API: &str = "https://huggingface.co/api";

pub struct HfClient {
    http: Client,
    token: Option<String>,
}

impl HfClient {
    pub fn new(token: Option<String>) -> Self {
        Self {
            http: Client::builder()
                .timeout(std::time::Duration::from_secs(15))
                .build()
                .expect("Failed to build HTTP client"),
            token,
        }
    }

    /// Try to find token from env or file
    pub fn with_auto_token() -> Self {
        let token = std::env::var("HF_TOKEN")
            .or_else(|_| std::env::var("HUGGING_FACE_HUB_TOKEN"))
            .ok()
            .or_else(|| {
                let path = dirs::home_dir()?.join(".cache/huggingface/token");
                std::fs::read_to_string(path).ok().map(|s| s.trim().to_string())
            });
        Self::new(token)
    }

    fn auth_header(&self) -> Option<String> {
        self.token.as_ref().map(|t| format!("Bearer {t}"))
    }

    /// Get full model info with provider mapping
    pub async fn model_info(&self, model_id: &str) -> Result<Value> {
        let url = format!("{HF_API}/models/{model_id}?expand[]=inferenceProviderMapping&expand[]=inference");
        let mut req = self.http.get(&url);
        if let Some(auth) = self.auth_header() {
            req = req.header("Authorization", auth);
        }
        let resp = req.send().await?;
        if resp.status().as_u16() == 404 {
            return Err(HfpError::ModelNotFound(model_id.to_string()));
        }
        Ok(resp.json().await?)
    }

    /// Search models by query
    pub async fn search_models(&self, query: &str, limit: u32) -> Result<Vec<Value>> {
        let url = format!(
            "{HF_API}/models?search={}&limit={}&expand[]=inferenceProviderMapping&sort=likes&direction=-1",
            urlencoding::encode(query), limit
        );
        let mut req = self.http.get(&url);
        if let Some(auth) = self.auth_header() {
            req = req.header("Authorization", auth);
        }
        Ok(req.send().await?.json().await?)
    }

    /// List models by provider
    pub async fn models_by_provider(&self, provider: &str, task: Option<&str>, limit: u32) -> Result<Vec<Value>> {
        let mut url = format!(
            "{HF_API}/models?inference_provider={provider}&limit={limit}&sort=likes&direction=-1"
        );
        if let Some(t) = task {
            url.push_str(&format!("&pipeline_tag={t}"));
        }
        let mut req = self.http.get(&url);
        if let Some(auth) = self.auth_header() {
            req = req.header("Authorization", auth);
        }
        Ok(req.send().await?.json().await?)
    }
}

/// Parse raw API JSON into our Model type
pub fn parse_model(data: &Value) -> Option<Model> {
    let id = data.get("id")?.as_str()?.to_string();
    let pipeline_tag = data.get("pipeline_tag").and_then(|v| v.as_str()).map(String::from);
    let likes = data.get("likes").and_then(|v| v.as_u64()).unwrap_or(0);
    let downloads = data.get("downloads").and_then(|v| v.as_u64()).unwrap_or(0);
    let inference_status = data.get("inference").and_then(|v| v.as_str()).map(String::from);

    let mut providers = Vec::new();
    if let Some(ipm) = data.get("inferenceProviderMapping").and_then(|v| v.as_object()) {
        for (name, info) in ipm {
            providers.push(ProviderInfo {
                name: name.clone(),
                status: match info.get("status").and_then(|v| v.as_str()) {
                    Some("live") => ProviderStatus::Live,
                    Some("staging") => ProviderStatus::Staging,
                    _ => ProviderStatus::Unknown,
                },
                task: info.get("task").and_then(|v| v.as_str()).unwrap_or("").to_string(),
                provider_id: info.get("providerId").and_then(|v| v.as_str()).unwrap_or("").to_string(),
                // Pricing enriched later
                input_price_per_m: None,
                output_price_per_m: None,
                throughput_tps: None,
                latency_s: None,
                context_window: None,
                supports_tools: None,
                supports_structured: None,
            });
        }
    }

    Some(Model {
        id,
        pipeline_tag,
        likes,
        downloads,
        inference_status,
        providers,
        variants: Vec::new(), // filled in by variant search
    })
}

// â”€â”€â”€ crates/hf-providers-core/src/snippet.rs â”€â”€â”€

use crate::model::Model;
use crate::provider::ProviderInfo;

pub enum Lang { Python, Curl, Javascript }

pub fn generate(model: &Model, provider: &ProviderInfo, lang: Lang) -> String {
    let model_id = &model.id;
    let prov = &provider.name;

    match lang {
        Lang::Python => format!(r#"from huggingface_hub import InferenceClient

client = InferenceClient(provider="{prov}")
response = client.chat.completions.create(
    model="{model_id}",
    messages=[{{"role": "user", "content": "Hello!"}}]
)
print(response.choices[0].message.content)"#),

        Lang::Curl => format!(r#"curl -X POST https://router.huggingface.co/v1/chat/completions \
  -H "Authorization: Bearer $HF_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{{"model":"{model_id}:{prov}","messages":[{{"role":"user","content":"Hello!"}}]}}'"#),

        Lang::Javascript => format!(r#"import {{ InferenceClient }} from "@huggingface/inference";

const client = new InferenceClient(process.env.HF_TOKEN);
const result = await client.chatCompletion({{
  model: "{model_id}",
  provider: "{prov}",
  messages: [{{ role: "user", content: "Hello!" }}],
}});
console.log(result.choices[0].message.content);"#),
    }
}

// â”€â”€â”€ crates/hf-providers-core/src/pricing.rs â”€â”€â”€

// TODO v0.2: Scrape/parse pricing from https://huggingface.co/inference/models
// For now, pricing comes enriched from the inference models page
// which returns structured JSON we can parse.

// The pricing page data includes per-model-per-provider:
// - input_price, output_price ($/1M tokens)
// - latency_s (time to first token)
// - throughput_tps (tokens per second)
// - context_window
// - supports_tools, supports_structured_output



// crates/hf-providers-cli/src/main.rs
//
// Terminal UI follows terminus style:
// - No emoji, unicode symbols only (â—, â—‹, â—, âœ“, âœ—, â”€)
// - Muted 256-color palette, mostly grays
// - Space-aligned columns, no box borders
// - Lowercase section headers
// - High information density

use clap::{Parser, Subcommand};
use console::{style, Style, Term};
use hf_providers_core::{
    api::{HfClient, parse_model},
    model::Model,
    provider::{ProviderInfo, Readiness, PROVIDERS, ProviderKind},
    snippet::{self, Lang},
};

// â”€â”€â”€ Palette â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fn s_header() -> Style  { Style::new().color256(245) }          // medium gray
fn s_dim() -> Style     { Style::new().color256(240) }          // dark gray
fn s_tree() -> Style    { Style::new().color256(238) }          // very dark gray
fn s_hint() -> Style    { Style::new().color256(236) }          // almost invisible
fn s_hot() -> Style     { Style::new().color256(70) }           // muted green
fn s_warm() -> Style    { Style::new().color256(214) }          // yellow-orange
fn s_cold() -> Style    { Style::new().color256(240) }          // dark gray
fn s_err() -> Style     { Style::new().color256(131) }          // muted red
fn s_price() -> Style   { Style::new().color256(109) }          // muted cyan
fn s_bold() -> Style    { Style::new().bold() }

fn sep(width: usize) -> String {
    s_tree().apply_to("â”€".repeat(width)).to_string()
}

fn readiness_str(r: Readiness) -> String {
    match r {
        Readiness::Hot         => format!("{}", s_hot().apply_to("â— hot")),
        Readiness::Warm        => format!("{}", s_warm().apply_to("â— warm")),
        Readiness::Cold        => format!("{}", s_cold().apply_to("â—‹ cold")),
        Readiness::Unavailable => format!("{}", s_err().apply_to("âœ— unavail")),
    }
}

fn bool_str(v: Option<bool>) -> String {
    match v {
        Some(true) => format!("{}", s_hot().apply_to("âœ“")),
        _ => format!("{}", s_tree().apply_to("â”€")),
    }
}

fn fmt_count(n: u64) -> String {
    if n >= 1_000_000 { format!("{:.1}M", n as f64 / 1_000_000.0) }
    else if n >= 1_000 { format!("{:.1}k", n as f64 / 1_000.0) }
    else { n.to_string() }
}

fn fmt_price(v: Option<f64>) -> String {
    match v {
        Some(p) => format!("{}", s_price().apply_to(format!("${:.2}", p))),
        None => format!("{}", s_tree().apply_to("â”€")),
    }
}

fn fmt_tput(v: Option<f64>) -> String {
    match v {
        Some(t) => format!("{:.0} t/s", t),
        None => format!("{}", s_tree().apply_to("â”€")),
    }
}

// â”€â”€â”€ CLI Args â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#[derive(Parser)]
#[command(
    name = "hfp",
    about = "Find out how to run any Hugging Face model",
    version,
    after_help = "examples:\n  \
        hfp deepseek-r1\n  \
        hfp meta-llama/Llama-3.3-70B-Instruct\n  \
        hfp flux.1-dev\n  \
        hfp deepseek-r1 --cheapest\n  \
        hfp providers groq\n  \
        hfp run deepseek-r1"
)]
struct Cli {
    query: Option<String>,

    #[command(subcommand)]
    command: Option<Commands>,

    #[arg(long)]
    cheapest: bool,

    #[arg(long)]
    fastest: bool,

    #[arg(long)]
    tools: bool,

    #[arg(long)]
    hot: bool,

    #[arg(long, short)]
    json: bool,
}

#[derive(Subcommand)]
enum Commands {
    /// Code snippet to run a model
    Run {
        model: String,
        #[arg(long, short, default_value = "python")]
        lang: String,
        #[arg(long, short)]
        provider: Option<String>,
        #[arg(long)]
        fastest: bool,
        #[arg(long)]
        cheapest: bool,
    },
    /// List providers or browse a provider's models
    Providers {
        name: Option<String>,
        #[arg(long, short)]
        task: Option<String>,
    },
    /// Live status across providers
    Status {
        model: String,
        #[arg(long, short)]
        watch: Option<u64>,
    },
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();
    let client = HfClient::with_auto_token();

    match cli.command {
        Some(Commands::Run { model, lang, provider, fastest, cheapest }) => {
            cmd_run(&client, &model, &lang, provider.as_deref(), fastest, cheapest).await?;
        }
        Some(Commands::Providers { name, task }) => {
            cmd_providers(&client, name.as_deref(), task.as_deref()).await?;
        }
        Some(Commands::Status { model, watch }) => {
            cmd_status(&client, &model, watch).await?;
        }
        None => {
            if let Some(query) = cli.query {
                cmd_search(&client, &query, &cli).await?;
            } else {
                use clap::CommandFactory;
                Cli::command().print_help()?;
            }
        }
    }
    Ok(())
}

// â”€â”€â”€ Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async fn cmd_search(client: &HfClient, query: &str, opts: &Cli) -> anyhow::Result<()> {
    let term = Term::stderr();
    term.write_line(&format!("{}", s_dim().apply_to("searching...")))?;

    // Try exact match
    let model = match client.model_info(query).await {
        Ok(data) => parse_model(&data),
        Err(_) => None,
    };

    let model = if let Some(m) = model {
        term.clear_last_lines(1)?;
        m
    } else {
        let results = client.search_models(query, 15).await?;
        term.clear_last_lines(1)?;

        if results.is_empty() {
            eprintln!("{}", s_err().apply_to(format!("error: no models found for '{}'", query)));
            eprintln!();
            eprintln!("{}", s_dim().apply_to("  Try the full model ID, e.g. deepseek-ai/DeepSeek-R1"));
            eprintln!("{}", s_dim().apply_to(format!("  Or broaden search: hfp {}", query.split('-').next().unwrap_or(query))));
            return Ok(());
        }

        let models: Vec<Model> = results.iter().filter_map(|r| parse_model(r)).collect();

        if models.is_empty() {
            eprintln!("{}", s_err().apply_to("error: could not parse results"));
            return Ok(());
        }

        // Multiple results: show compact list unless there's one clear winner
        let has_clear_winner = models.len() == 1
            || (models[0].providers.len() > 0 && models[0].likes > models.get(1).map(|m| m.likes).unwrap_or(0) * 5);

        if !has_clear_winner && models.len() > 1 {
            print_search_results(query, &models);
            return Ok(());
        }

        models.into_iter()
            .find(|m| !m.providers.is_empty())
            .unwrap_or_else(|| {
                results.iter().filter_map(|r| parse_model(r)).next().unwrap()
            })
    };

    if opts.json {
        println!("{}", serde_json::to_string_pretty(&model)?);
        return Ok(());
    }

    // Search for variants
    let core = extract_core_name(&model.id);
    let variant_results = client.search_models(&core, 15).await.unwrap_or_default();
    let variants: Vec<Model> = variant_results.iter()
        .filter_map(|r| parse_model(r))
        .filter(|m| m.id != model.id)
        .collect();

    print_model_full(&model, &variants, opts);
    Ok(())
}

// â”€â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async fn cmd_run(
    client: &HfClient, query: &str, lang: &str,
    provider: Option<&str>, fastest: bool, cheapest: bool,
) -> anyhow::Result<()> {
    let data = client.model_info(query).await
        .or_else(|_| async {
            let results = client.search_models(query, 1).await?;
            results.into_iter().next()
                .ok_or(hf_providers_core::HfpError::ModelNotFound(query.to_string()))
        }.await)?;

    let model = parse_model(&data)
        .ok_or_else(|| anyhow::anyhow!("could not parse model data"))?;

    let chosen = if let Some(name) = provider {
        model.providers.iter().find(|p| p.name == name)
    } else if fastest {
        model.fastest()
    } else {
        // Default: cheapest, fallback to first
        model.cheapest().or(model.providers.first())
    };

    let prov = chosen.ok_or_else(|| anyhow::anyhow!("no providers available"))?;

    let l = match lang {
        "curl" => Lang::Curl,
        "js" | "javascript" => Lang::Javascript,
        _ => Lang::Python,
    };

    // One comment line for context, then pure code
    println!("{}", s_dim().apply_to(format!("# {} via {} ({})",
        model.id, prov.name,
        if cheapest || provider.is_none() { "cheapest" } else if fastest { "fastest" } else { "selected" }
    )));
    println!();
    println!("{}", snippet::generate(&model, prov, l));
    Ok(())
}

// â”€â”€â”€ Providers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async fn cmd_providers(client: &HfClient, name: Option<&str>, task: Option<&str>) -> anyhow::Result<()> {
    match name {
        Some(prov) => {
            let results = client.models_by_provider(prov, task, 20).await?;
            let models: Vec<Model> = results.iter().filter_map(|r| parse_model(r)).collect();

            let p = PROVIDERS.iter().find(|p| p.id == prov);
            let display = p.map(|p| p.display_name).unwrap_or(prov);
            let kind = p.map(|p| match p.kind {
                ProviderKind::InferenceProvider => "serverless GPU",
                ProviderKind::HfInference => "HF CPU",
            }).unwrap_or("");

            println!();
            println!("{}  {}", s_bold().apply_to(format!("{} â€” {}", prov, display)),
                s_dim().apply_to(kind));
            println!("{}", sep(64));

            for m in &models {
                let tag = m.pipeline_tag.as_deref().unwrap_or("");
                println!("  {:<45} {:<18} {}",
                    s_bold().apply_to(&m.id),
                    s_dim().apply_to(tag),
                    s_dim().apply_to(format!("â™¥ {}", fmt_count(m.likes)))
                );
            }

            println!("{}", sep(64));
            println!("{}", s_hint().apply_to(format!("  {} models   hfp <model> for details", models.len())));
            println!();
        }
        None => {
            println!();
            println!("{}", s_header().apply_to("inference providers"));
            println!("{}", sep(64));

            for p in PROVIDERS {
                let kind_str = match p.kind {
                    ProviderKind::InferenceProvider => s_dim().apply_to("serverless GPU"),
                    ProviderKind::HfInference => s_warm().apply_to("HF CPU"),
                };
                println!("  {:<18} {:<16} {}",
                    s_bold().apply_to(p.id), s_dim().apply_to(p.display_name), kind_str);
            }

            println!("{}", sep(64));
            println!("{}", s_hint().apply_to(format!("  {} providers   hfp providers <name> for models", PROVIDERS.len())));
            println!();
        }
    }
    Ok(())
}

// â”€â”€â”€ Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async fn cmd_status(client: &HfClient, query: &str, watch: Option<u64>) -> anyhow::Result<()> {
    // Pulse animation frames
    let pulse = ['âœ±', 'âœ¦', 'Â·', 'âœ¦'];
    let mut frame: usize = 0;

    loop {
        let data = client.model_info(query).await?;
        let model = parse_model(&data)
            .ok_or_else(|| anyhow::anyhow!("could not parse model"))?;

        let term = Term::stderr();
        if watch.is_some() {
            term.clear_screen()?;
        }

        let refresh_indicator = if watch.is_some() {
            format!("  {}", s_warm().apply_to(format!("{} refreshing...", pulse[frame % pulse.len()])))
        } else {
            String::new()
        };

        let now = chrono::Local::now().format("%H:%M:%S");
        println!();
        println!("{}  {}{}",
            s_bold().apply_to(&model.id),
            s_dim().apply_to(now),
            refresh_indicator
        );
        println!("{}", sep(64));

        for p in &model.providers {
            let r = p.readiness();
            let ttft = p.latency_s
                .map(|l| format!("~{:.0}ms TTFT", l * 1000.0))
                .unwrap_or_else(|| {
                    if r == Readiness::Cold { "unavailable".to_string() }
                    else { "â”€".to_string() }
                });

            println!("  {:<16} {:<12} {}",
                &p.name,
                readiness_str(r),
                s_dim().apply_to(ttft)
            );
        }

        println!("{}", sep(64));

        match watch {
            Some(secs) => {
                println!("{}", s_hint().apply_to(format!("  â†» {}s", secs)));
                frame += 1;
                tokio::time::sleep(std::time::Duration::from_secs(secs)).await;
            }
            None => {
                println!();
                break;
            }
        }
    }
    Ok(())
}

// â”€â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fn print_model_full(model: &Model, variants: &[Model], opts: &Cli) {
    let tag = model.pipeline_tag.as_deref().unwrap_or("unknown");
    let param = Model::param_hint(&model.id).unwrap_or_default();
    let inf = model.inference_status.as_deref().unwrap_or("unknown");

    // Header: one line model identity, one line stats
    println!();
    println!("{}  {}  {}",
        s_bold().apply_to(&model.id),
        s_dim().apply_to(tag),
        if param.is_empty() { String::new() } else { s_dim().apply_to(&param).to_string() }
    );
    println!("{}",
        s_dim().apply_to(format!("â™¥ {}  â†“ {}  inference: {}",
            fmt_count(model.likes), fmt_count(model.downloads), inf))
    );

    // â”€â”€ Serverless providers â”€â”€

    let mut providers: Vec<&ProviderInfo> = model.providers.iter().collect();

    if opts.tools {
        providers.retain(|p| p.supports_tools == Some(true));
    }
    if opts.hot {
        providers.retain(|p| p.readiness() == Readiness::Hot);
    }
    if opts.cheapest {
        providers.sort_by(|a, b|
            a.output_price_per_m.unwrap_or(f64::MAX)
                .partial_cmp(&b.output_price_per_m.unwrap_or(f64::MAX))
                .unwrap_or(std::cmp::Ordering::Equal));
    } else if opts.fastest {
        providers.sort_by(|a, b|
            b.throughput_tps.unwrap_or(0.0)
                .partial_cmp(&a.throughput_tps.unwrap_or(0.0))
                .unwrap_or(std::cmp::Ordering::Equal));
    } else {
        providers.sort_by(|a, b|
            a.readiness().cmp(&b.readiness()).then(a.name.cmp(&b.name)));
    }

    println!();
    if providers.is_empty() {
        println!("{}", s_header().apply_to("serverless providers"));
        println!("{}", sep(64));
        println!("  {}", s_dim().apply_to("none available"));
        println!("{}", sep(64));
    } else {
        println!("{}", s_header().apply_to("serverless providers"));
        println!("{}", sep(64));

        // Column header
        println!("  {:<16} {:<10} {:<9} {:<9} {:<9} {:<6} {}",
            s_dim().apply_to("Provider"),
            s_dim().apply_to("Status"),
            s_dim().apply_to("In $/1M"),
            s_dim().apply_to("Out $/1M"),
            s_dim().apply_to("Tput"),
            s_dim().apply_to("Tools"),
            s_dim().apply_to("JSON"),
        );

        for p in &providers {
            println!("  {:<16} {:<10} {:<9} {:<9} {:<9} {:<6} {}",
                &p.name,
                readiness_str(p.readiness()),
                fmt_price(p.input_price_per_m),
                fmt_price(p.output_price_per_m),
                fmt_tput(p.throughput_tps),
                bool_str(p.supports_tools),
                bool_str(p.supports_structured),
            );
        }

        println!("{}", sep(64));

        // Summary
        let mut summary = Vec::new();
        if let Some(c) = model.cheapest() {
            let price = match (c.input_price_per_m, c.output_price_per_m) {
                (Some(i), Some(o)) => format!(" (${:.2}/${:.2})", i, o),
                _ => String::new(),
            };
            summary.push(format!("cheapest: {}{}", c.name, price));
        }
        if let Some(f) = model.fastest() {
            let tput = f.throughput_tps.map(|t| format!(" ({:.0} t/s)", t)).unwrap_or_default();
            summary.push(format!("fastest: {}{}", f.name, tput));
        }
        if !summary.is_empty() {
            println!("  {}", s_dim().apply_to(summary.join("   ")));
        }
    }

    // â”€â”€ Dedicated endpoint â”€â”€

    println!();
    println!("{}", s_header().apply_to("dedicated endpoint"));
    println!("  {}", s_dim().apply_to(format!("Deploy at huggingface.co/{} â†’ Deploy â†’ Inference Endpoints", model.id)));
    if let Some(ref hint) = Model::param_hint(&model.id) {
        let est = estimate_hourly(hint);
        if !est.is_empty() {
            println!("  {}", s_dim().apply_to(est));
        }
    }

    // â”€â”€ Local â”€â”€

    println!();
    println!("{}", s_header().apply_to("local"));
    println!("  {}", s_dim().apply_to(format!("vllm serve {}", model.id)));
    if let Some(ref hint) = Model::param_hint(&model.id) {
        let vram = estimate_vram(hint);
        if !vram.is_empty() {
            println!("  {}", s_dim().apply_to(format!("VRAM: ~{}", vram)));
        }
    }

    // â”€â”€ Variants â”€â”€

    if !variants.is_empty() {
        println!();
        println!("{}", s_header().apply_to("variants"));
        println!("{}", sep(64));

        for v in variants.iter().take(10) {
            let pcount = v.providers.len();
            let param = Model::param_hint(&v.id).unwrap_or_default();
            let prov_str = if pcount > 0 {
                format!("{} provider{}", pcount, if pcount != 1 { "s" } else { "" })
            } else {
                "0 providers".to_string()
            };
            println!("  {:<48} {:<14} {}",
                &v.id,
                s_dim().apply_to(prov_str),
                if param.is_empty() { String::new() } else { s_dim().apply_to(&param).to_string() }
            );
        }

        println!("{}", sep(64));
    }

    // â”€â”€ Footer hints â”€â”€

    println!("{}", s_hint().apply_to(
        "  hfp <variant> for details   hfp run <model> for code snippets"
    ));
    println!();
}

fn print_search_results(query: &str, models: &[Model]) {
    println!();
    println!("{}", s_header().apply_to(format!("search: {}", query)));
    println!("{}", sep(64));

    for m in models.iter().take(15) {
        let pcount = m.providers.len();
        let tag = m.pipeline_tag.as_deref().unwrap_or("");
        let param = Model::param_hint(&m.id).unwrap_or_default();
        let prov_str = if pcount > 0 {
            s_hot().apply_to(format!("{} providers", pcount)).to_string()
        } else {
            s_dim().apply_to("0 providers").to_string()
        };

        println!("  {:<45} {:<18} {:<14} {}",
            s_bold().apply_to(&m.id),
            s_dim().apply_to(tag),
            prov_str,
            if param.is_empty() { String::new() } else { s_dim().apply_to(&param).to_string() }
        );
    }

    println!("{}", sep(64));
    println!("{}", s_hint().apply_to(format!("  {} results   hfp <model-id> for details", models.len())));
    println!();
}

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fn estimate_vram(param_hint: &str) -> String {
    let b: f64 = param_hint.trim_end_matches('B').trim_end_matches('b')
        .parse().unwrap_or(0.0);
    if b <= 0.0 { return String::new(); }
    format!("{:.0}GB FP16 / {:.0}GB INT8 / {:.0}GB INT4", b * 2.0, b, b * 0.5)
}

fn estimate_hourly(param_hint: &str) -> String {
    let b: f64 = param_hint.trim_end_matches('B').trim_end_matches('b')
        .parse().unwrap_or(0.0);
    if b <= 0.0 { return String::new(); }
    // Very rough: small models on A10G, large on A100
    if b <= 13.0 {
        "~$1.10/hr on A10G 24GB".to_string()
    } else if b <= 70.0 {
        "~$4.50/hr on A100 80GB".to_string()
    } else {
        "~$18/hr on 4xA100 80GB (multi-GPU)".to_string()
    }
}

fn extract_core_name(model_id: &str) -> String {
    let name = model_id.split('/').last().unwrap_or(model_id);
    let base = name
        .replace("-Instruct", "").replace("-instruct", "")
        .replace("-Chat", "").replace("-chat", "")
        .replace("-FP8", "").replace("-fp8", "");
    let parts: Vec<&str> = base.split('-').collect();
    let core = parts.iter().take(3).copied().collect::<Vec<_>>().join("-");
    if let Some(org) = model_id.split('/').next() {
        if model_id.contains('/') {
            return format!("{}/{}", org, core);
        }
    }
    core
}
