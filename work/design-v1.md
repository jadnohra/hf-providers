# hf-providers â€” Design Doc

## The One-Liner

**"I want to run this model â€” what are my options?"**

A CLI + library that answers this question instantly, addressing four pain points that the HF community consistently struggles with.

---

## Pain Points We Solve

| # | Pain | Current Reality | Our Answer |
|---|------|----------------|------------|
| 1 | **"Can I run this model?"** | Browse web UI checkboxes, no CLI | `hfp run deepseek-r1` â†’ full answer |
| 2 | **"What kind of running?"** | Inference Providers / HF Inference / Endpoints / Local all blur together | Clear categories with color-coded output |
| 3 | **"Is it ready NOW?"** | API only says "warm" or nothing | Live status + cold start estimates |
| 4 | **"What will it cost?"** | Pricing scattered, surprise bills | Side-by-side cost comparison |

---

## CLI Design â€” `hfp`

Short name. Fast to type. Memorable.

### Core Command: Just Give Me the Answer

```bash
$ hfp deepseek-r1
```

```
â•­â”€ deepseek-ai/DeepSeek-R1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Text Generation Â· ğŸ”¥ Trending Â· â™¥ 48k Â· â†“ 12M                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

âš¡ SERVERLESS INFERENCE (API call, pay-per-token, instant)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider     â”‚ Status â”‚ In $/1M  â”‚ Out $/1M â”‚ Tput   â”‚ Tools â”‚ JSON â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ novita       â”‚ ğŸŸ¢ hot â”‚ $0.56    â”‚ $2.00    â”‚ 27 t/s â”‚  âœ“    â”‚      â”‚
â”‚ sambanova    â”‚ ğŸŸ¢ hot â”‚ â€”        â”‚ â€”        â”‚ 204t/s â”‚       â”‚  âœ“   â”‚
â”‚ hyperbolic   â”‚ ğŸŸ¢ hot â”‚ $2.00    â”‚ $2.00    â”‚ 45 t/s â”‚       â”‚      â”‚
â”‚ together     â”‚ ğŸŸ¡ warmâ”‚ $3.00    â”‚ $7.00    â”‚ 34 t/s â”‚       â”‚  âœ“   â”‚
â”‚ fireworks-ai â”‚ âš« coldâ”‚ â€”        â”‚ â€”        â”‚ â€”      â”‚       â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
  cheapest: novita ($0.56/$2.00)  fastest: sambanova (204 t/s)

ğŸ–¥  DEDICATED ENDPOINT (your own GPU, pay-per-hour)
   Deploy via: huggingface.co/deepseek-ai/DeepSeek-R1 â†’ Deploy â†’ Inference Endpoints
   Estimated: ~$4.50/hr on A100 80GB

ğŸ’» LOCAL (free, your hardware)
   ollama run deepseek-r1          # if available
   vllm serve deepseek-ai/DeepSeek-R1
   transformers: AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1")
   VRAM needed: ~160GB FP16 / ~80GB INT8

â•­â”€ ğŸ“¦ Variants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ deepseek-ai/DeepSeek-R1-0528          5 providers  671B   latest   â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Qwen-32B  2 providers  32B  â­    â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Llama-70B 4 providers  70B        â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Qwen-14B  2 providers  14B        â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Qwen-7B   1 provider   7B         â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 1 provider   1.5B       â”‚
â”‚ deepseek-ai/DeepSeek-R1-Distill-Llama-8B  1 provider   8B         â”‚
â”‚                                                                     â”‚
â”‚ Run: hfp deepseek-r1-distill-qwen-32b for details                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ’¡ Quick start:
   hfp run deepseek-ai/DeepSeek-R1    # copy-paste code snippet
```

### Quick Start Snippet

```bash
$ hfp run deepseek-r1
```

```
# Cheapest provider (novita, ~$0.56/1M input tokens)
from huggingface_hub import InferenceClient
client = InferenceClient(provider="novita")
r = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Or OpenAI-compatible (auto-routes to fastest)
from openai import OpenAI
client = OpenAI(base_url="https://router.huggingface.co/v1", api_key="hf_...")
client.chat.completions.create(model="deepseek-ai/DeepSeek-R1:fastest", ...)

# curl
curl -X POST https://router.huggingface.co/v1/chat/completions \
  -H "Authorization: Bearer $HF_TOKEN" \
  -d '{"model":"deepseek-ai/DeepSeek-R1:cheapest","messages":[...]}'
```

### Sorting & Filtering

```bash
hfp deepseek-r1 --cheapest          # sort by price
hfp deepseek-r1 --fastest           # sort by throughput
hfp deepseek-r1 --tools             # only providers with tool calling
hfp deepseek-r1 --json              # machine-readable output
hfp deepseek-r1 --hot               # only providers with hot/ready status
```

### Provider-Centric Queries

```bash
hfp providers                       # list all 15+ providers
hfp providers groq                  # what does Groq serve?
hfp providers groq --task chat      # Groq's chat models
hfp providers --compare llama-70b   # side-by-side all providers for one model
```

### Status / Health

```bash
hfp status deepseek-r1              # live status across all providers
hfp status deepseek-r1 --watch      # auto-refresh every 5s
```

```
deepseek-ai/DeepSeek-R1 â€” status at 14:32:01
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Provider     â”‚ Status â”‚ Response Time    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ novita       â”‚ ğŸŸ¢ hot â”‚ ~890ms TTFT      â”‚
â”‚ sambanova    â”‚ ğŸŸ¢ hot â”‚ ~450ms TTFT      â”‚
â”‚ together     â”‚ ğŸŸ¡ warmâ”‚ ~780ms TTFT      â”‚
â”‚ fireworks-ai â”‚ âš« coldâ”‚ unavailable      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†» refreshing in 5s...
```

### Fuzzy Search

```bash
hfp llama 3.3                       # finds meta-llama/Llama-3.3-70B-Instruct
hfp flux                            # finds FLUX.1-dev, FLUX.1-schnell, etc.
hfp qwen coder                      # finds Qwen3-Coder variants
hfp "whisper large"                  # finds openai/whisper-large-v3
```

---

## Library Design

### Python (via PyO3 bindings)

```python
from hf_providers import Model, providers

# The core question
model = Model("deepseek-r1")
model.name              # "deepseek-ai/DeepSeek-R1"
model.task              # "text-generation"
model.providers         # [Provider(name="novita", status="live", ...), ...]
model.variants          # [Model("DeepSeek-R1-0528"), Model("DeepSeek-R1-Distill-Qwen-32B"), ...]
model.cheapest          # Provider(name="novita", input_price=0.56, output_price=2.00)
model.fastest           # Provider(name="sambanova", throughput=204)

# Filter
model.providers_with(tools=True)
model.providers_with(status="hot")
model.providers_with(structured_output=True)

# Quick start code
print(model.snippet("python"))      # ready-to-paste code
print(model.snippet("curl"))
print(model.snippet("javascript"))

# Provider-centric
groq = providers.get("groq")
groq.models                         # all models on Groq
groq.models(task="text-generation") # filtered
groq.status("deepseek-r1")         # status for specific model
```

### Rust (native)

```rust
use hf_providers::{Model, Providers};

#[tokio::main]
async fn main() {
    let model = Model::search("deepseek-r1").await?;
    
    println!("{}", model.name);           // deepseek-ai/DeepSeek-R1
    
    for p in model.providers() {
        println!("{}: {} - {} t/s", 
            p.name, p.status, p.throughput.unwrap_or(0));
    }
    
    let cheapest = model.cheapest()?;
    let fastest = model.fastest()?;
}
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              hf-providers CLI               â”‚
â”‚           (Rust binary â€” `hfp`)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           hf-providers-core (Rust)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Model    â”‚ â”‚ Provider â”‚ â”‚ Status      â”‚ â”‚
â”‚  â”‚ Search   â”‚ â”‚ Registry â”‚ â”‚ Checker     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Pricing  â”‚ â”‚ Variant  â”‚ â”‚ Snippet     â”‚ â”‚
â”‚  â”‚ Fetcher  â”‚ â”‚ Grouper  â”‚ â”‚ Generator   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        HF Hub API (REST)                    â”‚
â”‚  GET /api/models?inference_provider=...     â”‚
â”‚  GET /api/models/{id}?expand[]=inf...       â”‚
â”‚  GET /inference/models (pricing page)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Python bindings (PyO3/maturin)         â”‚
â”‚           `pip install hf-providers`        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Sources

| Data | Source | Method |
|------|--------|--------|
| Provider mapping | `GET /api/models/{id}?expand[]=inferenceProviderMapping` | REST API |
| Model search | `GET /api/models?search=...` | REST API |
| Provider list | `GET /api/models?inference_provider=...` | REST API |
| Warm/cold status | `GET /api/models/{id}?expand[]=inference` | REST API |
| Pricing & metrics | `GET /inference/models` page | Parse structured data |
| Local run info | Model card metadata (params, arch) | REST API |

### Caching

- Cache model info for 5 minutes (providers don't change that fast)
- Cache pricing data for 1 hour
- Status checks are always live (that's the point)
- Cache in `~/.cache/hf-providers/`

---

## Project Structure

```
hf-providers/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ hf-providers-core/    # Library: API client, data types, logic
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ model.rs      # Model search, info, variants
â”‚   â”‚   â”‚   â”œâ”€â”€ provider.rs   # Provider registry, status
â”‚   â”‚   â”‚   â”œâ”€â”€ pricing.rs    # Cost data fetching
â”‚   â”‚   â”‚   â”œâ”€â”€ snippet.rs    # Code snippet generation
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.rs      # Local caching layer
â”‚   â”‚   â”‚   â””â”€â”€ api.rs        # HF Hub API client
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â””â”€â”€ hf-providers-cli/     # Binary: CLI interface
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ main.rs
â”‚       â”‚   â”œâ”€â”€ display.rs    # Terminal rendering (tables, colors)
â”‚       â”‚   â””â”€â”€ commands/
â”‚       â”‚       â”œâ”€â”€ search.rs # `hfp <query>`
â”‚       â”‚       â”œâ”€â”€ run.rs    # `hfp run <model>`
â”‚       â”‚       â”œâ”€â”€ status.rs # `hfp status <model>`
â”‚       â”‚       â””â”€â”€ providers.rs # `hfp providers [name]`
â”‚       â””â”€â”€ Cargo.toml
â”œâ”€â”€ python/                   # Python bindings
â”‚   â”œâ”€â”€ src/lib.rs            # PyO3 bindings
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ hf_providers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ py.typed
â””â”€â”€ tests/
```

### Key Rust Dependencies

```toml
[dependencies]
reqwest = { version = "0.12", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
clap = { version = "4", features = ["derive"] }
comfy-table = "7"           # Beautiful terminal tables
console = "0.15"            # Colors, styling
indicatif = "0.17"          # Progress bars
fuzzy-matcher = "0.3"       # Fuzzy model name matching
directories = "5"           # Cache paths
```

---

## Status Detection Strategy

The HF API is limited â€” it only gives "warm" or nothing. We augment this:

1. **API status** â€” `model_info(expand="inference")` â†’ warm/undefined
2. **Provider mapping status** â€” `inferenceProviderMapping[provider].status` â†’ "live"/"staging"
3. **Pricing page data** â€” latency numbers present = likely hot; `-` = likely cold/unavailable
4. **Optional: live probe** â€” `hfp status --probe` sends a tiny request to each provider and measures TTFT

Display as:
- ğŸŸ¢ **hot** â€” provider has latency data, status=live, inference=warm
- ğŸŸ¡ **warm** â€” status=live but no latency data (might have cold start)
- âš« **cold** â€” status=live but all metrics show `-` (probably needs spin-up)
- âŒ **unavailable** â€” not in provider mapping at all

---

## MVP Scope (v0.1)

1. `hfp <query>` â€” fuzzy search, show providers + status + pricing
2. `hfp run <model>` â€” code snippets for Python, curl, JS
3. `hfp providers` â€” list all providers
4. `hfp --json` â€” machine-readable output
5. Cache layer
6. Reads `$HF_TOKEN` / `~/.cache/huggingface/token`

## v0.2

7. `hfp status --watch` â€” live monitoring
8. `hfp status --probe` â€” actual latency measurement
9. Python bindings via PyO3
10. `brew install` / `cargo install` distribution

## v0.3

11. Local run detection (check if ollama has the model, estimate VRAM)
12. Cost calculator (`hfp cost deepseek-r1 --tokens 1M`)
13. Shell completions (bash, zsh, fish)
